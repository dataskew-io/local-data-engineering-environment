{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb841f13",
   "metadata": {},
   "source": [
    "# Local Data Engineering Workflow\n",
    "\n",
    "## Objective\n",
    "This notebook demonstrates a complete local data engineering workflow using:\n",
    "- **dlt** for data loading and transformation\n",
    "- **DuckDB** for embedded analytics\n",
    "- **Jupyter** for interactive development\n",
    "\n",
    "## Environment Setup\n",
    "Make sure you have activated your virtual environment and installed all dependencies from `requirements.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13c8eef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "Current working directory: /Users/adrianosanges/personal/local-data-engineering-environment/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import dlt\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89204929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ dlt pipeline initialized successfully!\n",
      "Pipeline name: local_data\n",
      "Destination: <dlt.destinations.duckdb(destination_type='duckdb', staging_dataset_name_layout='%s_staging', enable_dataset_name_normalization=True, info_tables_query_threshold=1000, truncate_tables_on_staging_destination_before_load=True, local_dir='/Users/adrianosanges/personal/local-data-engineering-environment/notebooks', pipeline_name='local_data', pipeline_working_dir='/Users/adrianosanges/.dlt/pipelines/local_data', create_indexes=False)>\n",
      "Dataset: my_data\n"
     ]
    }
   ],
   "source": [
    "# Initialize dlt pipeline with DuckDB destination\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"local_data\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"my_data\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ dlt pipeline initialized successfully!\")\n",
    "print(f\"Pipeline name: {pipeline.pipeline_name}\")\n",
    "print(f\"Destination: {pipeline.destination}\")\n",
    "print(f\"Dataset: {pipeline.dataset_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4289a013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded 10 records from CSV\n",
      "üìã Columns: ['date', 'product', 'category', 'quantity', 'price', 'region']\n",
      "\n",
      "üîç Data Quality Checks:\n",
      "- Null values: 0\n",
      "- Duplicate rows: 0\n",
      "- Date range: 2024-01-01 to 2024-01-05\n",
      "‚úÖ Data transformations completed successfully!\n",
      "\n",
      "‚úÖ Data loaded successfully!\n",
      "Load info: Pipeline local_data load step completed in 0.04 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset my_data\n",
      "The duckdb destination used duckdb:////Users/adrianosanges/personal/local-data-engineering-environment/notebooks/local_data.duckdb location to store data\n",
      "Load package 1750598657.8910341 is LOADED and contains no failed jobs\n",
      "Load info type: <class 'dlt.common.pipeline.LoadInfo'>\n",
      "\n",
      "üîó Database path: /Users/adrianosanges/personal/local-data-engineering-environment/notebooks/local_data.duckdb\n",
      "\n",
      "üìã Available tables:\n",
      "  table_schema           table_name\n",
      "0      my_data          sample_data\n",
      "1      my_data           _dlt_loads\n",
      "2      my_data  _dlt_pipeline_state\n",
      "3      my_data         _dlt_version\n",
      "\n",
      "üìä Records in table 'my_data.sample_data': 10\n",
      "\n",
      "üìã Sample data:\n",
      "                       date     product     category  quantity   price region  \\\n",
      "0 2024-01-01 01:00:00+01:00      Laptop  Electronics         5  1200.0  North   \n",
      "1 2024-01-01 01:00:00+01:00  Desk Chair    Furniture         3   250.0  South   \n",
      "2 2024-01-02 01:00:00+01:00  Smartphone  Electronics         8   800.0   East   \n",
      "\n",
      "   total_revenue  month day_of_week        _dlt_load_id         _dlt_id  \n",
      "0         6000.0      1      Monday  1750598657.8910341  jNLjSbWlqeNWrg  \n",
      "1          750.0      1      Monday  1750598657.8910341  wthmr2zuVM7F6A  \n",
      "2         6400.0      1     Tuesday  1750598657.8910341  WBBfVMm0xm9abw  \n"
     ]
    }
   ],
   "source": [
    "def load_csv_with_transformations():\n",
    "    \"\"\"\n",
    "    Load CSV data with basic transformations and data quality checks.\n",
    "    \"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(\"../data/sample.csv\")\n",
    "    \n",
    "    print(f\"üìä Loaded {len(df)} records from CSV\")\n",
    "    print(f\"üìã Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Data quality checks\n",
    "    print(\"\\nüîç Data Quality Checks:\")\n",
    "    print(f\"- Null values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"- Duplicate rows: {df.duplicated().sum()}\")\n",
    "    print(f\"- Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    \n",
    "    # Basic transformations\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['total_revenue'] = df['quantity'] * df['price']\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day_of_week'] = df['date'].dt.day_name()\n",
    "    \n",
    "    # Add data quality assertions\n",
    "    assert df['quantity'].min() > 0, \"Quantity should be positive\"\n",
    "    assert df['price'].min() > 0, \"Price should be positive\"\n",
    "    assert df['total_revenue'].min() > 0, \"Total revenue should be positive\"\n",
    "    \n",
    "    print(\"‚úÖ Data transformations completed successfully!\")\n",
    "    \n",
    "    # Yield records for dlt\n",
    "    for record in df.to_dict(orient=\"records\"):\n",
    "        yield record\n",
    "\n",
    "# Load data into dlt pipeline\n",
    "info = pipeline.run(\n",
    "    load_csv_with_transformations(),\n",
    "    table_name=\"sample_data\",\n",
    "    write_disposition=\"replace\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "print(f\"Load info: {info}\")\n",
    "print(f\"Load info type: {type(info)}\")\n",
    "\n",
    "# Get the database path and connect directly to DuckDB\n",
    "db_path = pipeline.sql_client().credentials.database\n",
    "print(f\"\\nüîó Database path: {db_path}\")\n",
    "\n",
    "# Connect to DuckDB and check tables with proper schema handling\n",
    "con = duckdb.connect(db_path)\n",
    "\n",
    "# Get all tables from all schemas\n",
    "all_tables = con.execute(\"\"\"\n",
    "    SELECT table_schema, table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_type = 'BASE TABLE' AND table_schema != 'information_schema'\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(f\"\\nüìã Available tables:\")\n",
    "print(all_tables)\n",
    "\n",
    "# Find our data table (exclude dlt internal tables)\n",
    "data_tables = all_tables[~all_tables['table_name'].str.startswith('_dlt')]\n",
    "if not data_tables.empty:\n",
    "    table_schema = data_tables.iloc[0]['table_schema']\n",
    "    table_name = data_tables.iloc[0]['table_name']\n",
    "    full_table_name = f\"{table_schema}.{table_name}\"\n",
    "    \n",
    "    row_count = con.execute(f\"SELECT COUNT(*) FROM {full_table_name}\").fetchone()[0]\n",
    "    print(f\"\\nüìä Records in table '{full_table_name}': {row_count}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    sample = con.execute(f\"SELECT * FROM {full_table_name} LIMIT 3\").fetchdf()\n",
    "    print(f\"\\nüìã Sample data:\")\n",
    "    print(sample)\n",
    "else:\n",
    "    print(\"\\n‚ùå No data tables found in the database\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "013c7f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Connected to DuckDB at: /Users/adrianosanges/personal/local-data-engineering-environment/notebooks/local_data.duckdb\n",
      "\n",
      "üìã Available tables:\n",
      "  table_schema           table_name\n",
      "0      my_data          sample_data\n",
      "1      my_data           _dlt_loads\n",
      "2      my_data  _dlt_pipeline_state\n",
      "3      my_data         _dlt_version\n",
      "\n",
      "üìä Using table: my_data.sample_data\n",
      "\n",
      "üìà Summary Statistics:\n",
      " total_records  total_quantity  total_revenue  avg_price             earliest_date               latest_date\n",
      "            10            65.0        21125.0      372.5 2024-01-01 01:00:00+01:00 2024-01-05 01:00:00+01:00\n",
      "\n",
      "üè∑Ô∏è Sales by Category:\n",
      "   category  record_count  total_quantity  total_revenue  avg_price\n",
      "Electronics             5            46.0        17350.0      530.0\n",
      "  Furniture             5            19.0         3775.0      215.0\n",
      "\n",
      "üåç Sales by Region:\n",
      "region  record_count  total_quantity  total_revenue  avg_price\n",
      " North             3            26.0         9750.0     583.33\n",
      "  East             2            20.0         7600.0     450.00\n",
      " South             2             7.0         2350.0     325.00\n",
      "  West             2             9.0          825.0     112.50\n",
      "South              1             3.0          600.0     200.00\n",
      "\n",
      "‚úÖ DuckDB connection closed\n"
     ]
    }
   ],
   "source": [
    "# Connect to DuckDB and query the loaded data\n",
    "# Get the database path from the pipeline\n",
    "db_path = pipeline.sql_client().credentials.database\n",
    "print(f\"üîó Connected to DuckDB at: {db_path}\")\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect(db_path)\n",
    "\n",
    "# Get available tables with proper schema handling\n",
    "all_tables = con.execute(\"\"\"\n",
    "    SELECT table_schema, table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_type = 'BASE TABLE' AND table_schema != 'information_schema'\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(f\"\\nüìã Available tables:\")\n",
    "print(all_tables)\n",
    "\n",
    "# Find our data table (exclude dlt internal tables)\n",
    "data_tables = all_tables[~all_tables['table_name'].str.startswith('_dlt')]\n",
    "\n",
    "if data_tables.empty:\n",
    "    print(\"‚ùå No data tables found. Please run the data loading cell first.\")\n",
    "else:\n",
    "    table_schema = data_tables.iloc[0]['table_schema']\n",
    "    table_name = data_tables.iloc[0]['table_name']\n",
    "    full_table_name = f\"{table_schema}.{table_name}\"\n",
    "    \n",
    "    print(f\"\\nüìä Using table: {full_table_name}\")\n",
    "    \n",
    "    # Query 1: Basic summary statistics\n",
    "    print(\"\\nüìà Summary Statistics:\")\n",
    "    summary_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        SUM(quantity) as total_quantity,\n",
    "        SUM(total_revenue) as total_revenue,\n",
    "        AVG(price) as avg_price,\n",
    "        MIN(date) as earliest_date,\n",
    "        MAX(date) as latest_date\n",
    "    FROM {full_table_name}\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_result = con.execute(summary_query).fetchdf()\n",
    "    print(summary_result.to_string(index=False))\n",
    "    \n",
    "    # Query 2: Sales by category\n",
    "    print(\"\\nüè∑Ô∏è Sales by Category:\")\n",
    "    category_query = f\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as record_count,\n",
    "        SUM(quantity) as total_quantity,\n",
    "        SUM(total_revenue) as total_revenue,\n",
    "        AVG(price) as avg_price\n",
    "    FROM {full_table_name}\n",
    "    GROUP BY category\n",
    "    ORDER BY total_revenue DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    category_result = con.execute(category_query).fetchdf()\n",
    "    print(category_result.to_string(index=False))\n",
    "    \n",
    "    # Query 3: Sales by region\n",
    "    print(\"\\nüåç Sales by Region:\")\n",
    "    region_query = f\"\"\"\n",
    "    SELECT \n",
    "        region,\n",
    "        COUNT(*) as record_count,\n",
    "        SUM(quantity) as total_quantity,\n",
    "        SUM(total_revenue) as total_revenue,\n",
    "        ROUND(AVG(price), 2) as avg_price\n",
    "    FROM {full_table_name}\n",
    "    GROUP BY region\n",
    "    ORDER BY total_revenue DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    region_result = con.execute(region_query).fetchdf()\n",
    "    print(region_result.to_string(index=False))\n",
    "\n",
    "con.close()\n",
    "print(\"\\n‚úÖ DuckDB connection closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d7ae415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results exported to CSV files in ../output/ directory\n",
      "üìÅ Files created:\n",
      "  - summary_statistics.csv\n",
      "  - sales_by_category.csv\n",
      "  - sales_by_region.csv\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"../output\", exist_ok=True)\n",
    "\n",
    "# Get database path and connect\n",
    "db_path = pipeline.sql_client().credentials.database\n",
    "con = duckdb.connect(db_path)\n",
    "\n",
    "# Get table name with proper schema handling\n",
    "all_tables = con.execute(\"\"\"\n",
    "    SELECT table_schema, table_name \n",
    "    FROM information_schema.tables \n",
    "    WHERE table_type = 'BASE TABLE' AND table_schema != 'information_schema'\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "data_tables = all_tables[~all_tables['table_name'].str.startswith('_dlt')]\n",
    "\n",
    "if not data_tables.empty:\n",
    "    table_schema = data_tables.iloc[0]['table_schema']\n",
    "    table_name = data_tables.iloc[0]['table_name']\n",
    "    full_table_name = f\"{table_schema}.{table_name}\"\n",
    "    \n",
    "    # Export summary results to CSV\n",
    "    summary_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        SUM(quantity) as total_quantity,\n",
    "        SUM(total_revenue) as total_revenue,\n",
    "        AVG(price) as avg_price\n",
    "    FROM {full_table_name}\n",
    "    \"\"\"\n",
    "    summary_result = con.execute(summary_query).fetchdf()\n",
    "    summary_result.to_csv(\"../output/summary_statistics.csv\", index=False)\n",
    "    \n",
    "    category_query = f\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as record_count,\n",
    "        SUM(total_revenue) as total_revenue\n",
    "    FROM {full_table_name}\n",
    "    GROUP BY category\n",
    "    ORDER BY total_revenue DESC\n",
    "    \"\"\"\n",
    "    category_result = con.execute(category_query).fetchdf()\n",
    "    category_result.to_csv(\"../output/sales_by_category.csv\", index=False)\n",
    "    \n",
    "    region_query = f\"\"\"\n",
    "    SELECT \n",
    "        region,\n",
    "        COUNT(*) as record_count,\n",
    "        SUM(total_revenue) as total_revenue\n",
    "    FROM {full_table_name}\n",
    "    GROUP BY region\n",
    "    ORDER BY total_revenue DESC\n",
    "    \"\"\"\n",
    "    region_result = con.execute(region_query).fetchdf()\n",
    "    region_result.to_csv(\"../output/sales_by_region.csv\", index=False)\n",
    "    \n",
    "    print(\"‚úÖ Results exported to CSV files in ../output/ directory\")\n",
    "    print(\"üìÅ Files created:\")\n",
    "    print(\"  - summary_statistics.csv\")\n",
    "    print(\"  - sales_by_category.csv\")\n",
    "    print(\"  - sales_by_region.csv\")\n",
    "else:\n",
    "    print(\"‚ùå No data tables found. Please run the data loading cell first.\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c6a680",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What we accomplished:\n",
    "1. ‚úÖ Set up dlt pipeline with DuckDB destination\n",
    "2. ‚úÖ Loaded and transformed sample data with quality checks\n",
    "3. ‚úÖ Performed comprehensive analytics using SQL\n",
    "4. ‚úÖ Exported results to CSV files\n",
    "5. ‚úÖ Implemented data quality monitoring\n",
    "\n",
    "### Next steps for enhancement:\n",
    "- Add more data sources (APIs, databases)\n",
    "- Implement incremental loading\n",
    "- Add data validation schemas\n",
    "- Create automated data quality alerts\n",
    "- Build interactive dashboards\n",
    "- Set up scheduled data processing\n",
    "\n",
    "### Key metrics from this analysis:\n",
    "- Total records processed: [See summary above]\n",
    "- Total revenue: [See summary above]\n",
    "- Top performing category: [See category analysis above]\n",
    "- Most active region: [See region analysis above]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
